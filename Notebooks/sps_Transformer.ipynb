{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1747130699971,
     "user": {
      "displayName": "surge",
      "userId": "10562721468625119372"
     },
     "user_tz": -330
    },
    "id": "z5U7w_tj4z8-"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 12150,
     "status": "ok",
     "timestamp": 1747130712123,
     "user": {
      "displayName": "surge",
      "userId": "10562721468625119372"
     },
     "user_tz": -330
    },
    "id": "Nn7gqi4w420A",
    "outputId": "bc35c133-2d3e-4928-eace-ef041dbfd166"
   },
   "outputs": [],
   "source": [
    "# !mkdir 'timit_dataset'\n",
    "# !unzip '/content/drive/MyDrive/sps/preprocessed_timit_dataset.zip' -d timit_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 16831,
     "status": "ok",
     "timestamp": 1747130728955,
     "user": {
      "displayName": "surge",
      "userId": "10562721468625119372"
     },
     "user_tz": -330
    },
    "id": "QKeSkrkl49vO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1747130728958,
     "user": {
      "displayName": "surge",
      "userId": "10562721468625119372"
     },
     "user_tz": -330
    },
    "id": "XAQ_0DQ08049"
   },
   "outputs": [],
   "source": [
    "AUDIO_ROOT_DIR = '/content/timit_dataset'\n",
    "DRIVE_PATH = '/content/drive/MyDrive/sps/'\n",
    "\n",
    "TRAIN_CSV_PATH = os.path.join(DRIVE_PATH, 'final_train_data_merged.csv')\n",
    "TEST_CSV_PATH = os.path.join(DRIVE_PATH, 'final_test_data_merged.csv')\n",
    "CKPT_PATH = os.path.join(DRIVE_PATH, 'sps_transformer_best_model.pth')\n",
    "\n",
    "# Audio Processing\n",
    "SAMPLE_RATE = 16000\n",
    "CLIP_SECONDS = 4.0\n",
    "WAV_LEN = int(SAMPLE_RATE * CLIP_SECONDS)\n",
    "\n",
    "# Audio Augmentations\n",
    "APPLY_AUGMENTATIONS = True\n",
    "AUGMENTATION_PROBABILITY = 0.2\n",
    "ADD_NOISE_PROBABILITY = 0.2\n",
    "NOISE_SNR_MIN = 5.0\n",
    "NOISE_SNR_MAX = 20.0\n",
    "\n",
    "# Wav2Vec2.0 Base Model\n",
    "PRETRAINED_W2V2 = 'facebook/wav2vec2-base-960h'\n",
    "W2V2_OUTPUT_DIM = 768\n",
    "FREEZE_ENCODER = True\n",
    "\n",
    "# --- Transformer Encoder Parameters ---\n",
    "TRANSFORMER_D_MODEL = W2V2_OUTPUT_DIM\n",
    "TRANSFORMER_NHEAD = 8\n",
    "TRANSFORMER_NUM_ENCODER_LAYERS = 3\n",
    "TRANSFORMER_DIM_FEEDFORWARD = 2048\n",
    "TRANSFORMER_DROPOUT = 0.1\n",
    "\n",
    "# Dropout (for heads)\n",
    "MODEL_DROPOUT_RATE = 0.3\n",
    "\n",
    "# Training Hyperparameters\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 3e-5\n",
    "OPTIMIZER_WEIGHT_DECAY = 0.01\n",
    "GRADIENT_CLIP_MAX_NORM = 1.0\n",
    "\n",
    "# Validation\n",
    "VAL_SPLIT_RATIO = 0.25\n",
    "VAL_SPLIT_SEED = 42\n",
    "\n",
    "# Task Configuration\n",
    "TASKS = {\n",
    "    'age': {'type': 'regression', 'loss_weight': 0.4},\n",
    "    'Gender': {'type': 'classification', 'loss_weight': 0.4},\n",
    "    'height': {'type': 'regression', 'loss_weight': 0.2}\n",
    "}\n",
    "\n",
    "# Head-Specific Hyperparameters\n",
    "HEAD_CONFIGS = {\n",
    "    'age': {'head_hidden_dim': 128, 'head_dropout_rate': 0.3},\n",
    "    'Gender': {'head_hidden_dim': 96, 'head_dropout_rate': 0.25},\n",
    "    'height': {'head_hidden_dim': 64, 'head_dropout_rate': 0.2}\n",
    "}\n",
    "\n",
    "# Mappings and Normalization Stats\n",
    "GENDER_MAP = {}\n",
    "NORM_STATS = { 'age': {'mean': 0.0, 'std': 1.0}, 'height': {'mean': 0.0, 'std': 1.0}}\n",
    "\n",
    "# DataLoader\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY = True if DEVICE == 'cuda' else False\n",
    "\n",
    "# Model Saving, Early Stopping, LR Scheduling\n",
    "MONITOR_METRIC_MODE = 'min'\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "LR_SCHEDULER_PATIENCE = 5\n",
    "LR_SCHEDULER_FACTOR = 0.1\n",
    "MIN_LR = 1e-6\n",
    "\n",
    "EVAL_LOSS_WEIGHTS = { 'age': 0.4, 'Gender': 0.4, 'height': 0.2 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1747130728960,
     "user": {
      "displayName": "surge",
      "userId": "10562721468625119372"
     },
     "user_tz": -330
    },
    "id": "C_r8UVpiDlOU"
   },
   "outputs": [],
   "source": [
    "class PadCrop:\n",
    "    def __init__(self, length, mode = 'train'):\n",
    "        self.length = length\n",
    "        self.mode = mode\n",
    "\n",
    "    def __call__(self, wav):\n",
    "        current_len = wav.shape[-1]\n",
    "        if current_len == self.length:\n",
    "            return wav\n",
    "        elif current_len > self.length:\n",
    "            start = random.randint(\n",
    "                0, current_len - self.length) if self.mode == 'train' else (current_len - self.length) // 2\n",
    "            wav = wav[..., start: start + self.length]\n",
    "        else:\n",
    "            pad_width = self.length - current_len\n",
    "            pad_left = pad_width // 2\n",
    "            pad_right = pad_width - pad_left\n",
    "            wav = F.pad(wav, (pad_left, pad_right), mode='constant', value=0.0)\n",
    "        return wav\n",
    "\n",
    "\n",
    "class AddGaussianNoise:\n",
    "    def __init__(self, min_snr_db= 5.0, max_snr_db= 20.0, p = 0.5):\n",
    "        self.min_snr_db = min_snr_db\n",
    "        self.max_snr_db = max_snr_db\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, waveform):\n",
    "        if random.random() < self.p:\n",
    "            if waveform.ndim > 1 and waveform.shape[0] > 1:\n",
    "                wav_data_for_power_calc = waveform[0, :]\n",
    "            elif waveform.ndim > 1 and waveform.shape[0] == 1:\n",
    "                wav_data_for_power_calc = waveform.squeeze(0)\n",
    "            elif waveform.ndim == 1:\n",
    "                wav_data_for_power_calc = waveform\n",
    "            else:\n",
    "                return waveform\n",
    "\n",
    "            signal_power = torch.mean(wav_data_for_power_calc**2)\n",
    "            if signal_power.item() < 1e-9:\n",
    "                return waveform\n",
    "\n",
    "            snr_db = random.uniform(self.min_snr_db, self.max_snr_db)\n",
    "            snr_linear = 10**(snr_db / 10.0)\n",
    "            noise_power = signal_power / snr_linear\n",
    "            noise = torch.randn_like(waveform) * torch.sqrt(noise_power)\n",
    "            return waveform + noise\n",
    "            \n",
    "        return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1747130728962,
     "user": {
      "displayName": "surge",
      "userId": "10562721468625119372"
     },
     "user_tz": -330
    },
    "id": "NWG1QKOrDomv"
   },
   "outputs": [],
   "source": [
    "class TimitDataset(Dataset):\n",
    "    def __init__(self, data_df, mode = 'train'):\n",
    "        self.data_df = data_df.reset_index(drop=True)\n",
    "        self.mode = mode\n",
    "        self.pad_crop = PadCrop(WAV_LEN, mode)\n",
    "        self.target_cols = list(TASKS.keys())\n",
    "        self.add_noise_transform = None\n",
    "        if self.mode == 'train' and APPLY_AUGMENTATIONS:\n",
    "            self.add_noise_transform = AddGaussianNoise(NOISE_SNR_MIN, NOISE_SNR_MAX, ADD_NOISE_PROBABILITY)\n",
    "\n",
    "    def __len__(self): \n",
    "      return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.data_df):\n",
    "          print(f\"Index {idx} out of bounds...\")\n",
    "          return None\n",
    "        row = self.data_df.iloc[idx]\n",
    "        wav_relative_path = row['FilePath']\n",
    "        full_wav_path = os.path.normpath(os.path.join(AUDIO_ROOT_DIR, wav_relative_path))\n",
    "        wav, sr = torchaudio.load(full_wav_path)\n",
    "        if sr != SAMPLE_RATE:\n",
    "           wav = torchaudio.transforms.Resample(sr, SAMPLE_RATE)(wav)\n",
    "        if wav.shape[0] > 1:\n",
    "          wav = torch.mean(wav, dim=0, keepdim=True)\n",
    "        if self.mode == 'train' and APPLY_AUGMENTATIONS and random.random() < AUGMENTATION_PROBABILITY:\n",
    "            if self.add_noise_transform: wav = self.add_noise_transform(wav)\n",
    "        wav = self.pad_crop(wav).squeeze(0)\n",
    "        if torch.isnan(wav).any():\n",
    "          print(f\"NaNs in wav {idx}.\")\n",
    "          return None\n",
    "        targets = {}\n",
    "        valid_item = True\n",
    "        for task_name, task_info in TASKS.items():\n",
    "            value = row[task_name]\n",
    "            if pd.isna(value):\n",
    "              print(f\"NaN target {task_name} at {idx}.\")\n",
    "              valid_item = False\n",
    "              break\n",
    "            if task_info['type'] == 'regression':\n",
    "                mean, std = NORM_STATS[task_name]['mean'], NORM_STATS[task_name]['std']\n",
    "                targets[task_name] = torch.tensor((value - mean) / (std if std > 1e-6 else 1.0), dtype=torch.float32)\n",
    "            elif task_info['type'] == 'classification':\n",
    "                targets[task_name] = torch.tensor(GENDER_MAP.get(str(value).upper(), 0), dtype=torch.long)\n",
    "        return (wav, targets) if valid_item else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1747130728983,
     "user": {
      "displayName": "surge",
      "userId": "10562721468625119372"
     },
     "user_tz": -330
    },
    "id": "WYq9h376DsEW"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "      return None\n",
    "      \n",
    "    wavs = [item[0] for item in batch]\n",
    "    target_dicts = [item[1] for item in batch]\n",
    "    padded_wavs = torch.nn.utils.rnn.pad_sequence(wavs, batch_first=True, padding_value=0.0)\n",
    "    collated_targets = {}\n",
    "    if target_dicts:\n",
    "        first_item_keys = target_dicts[0].keys()\n",
    "        for key in first_item_keys:\n",
    "            if all(key in d for d in target_dicts):\n",
    "                 collated_targets[key] = torch.stack([d[key] for d in target_dicts])\n",
    "    if not collated_targets and target_dicts:\n",
    "      return None\n",
    "\n",
    "    return padded_wavs, collated_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1747130728991,
     "user": {
      "displayName": "surge",
      "userId": "10562721468625119372"
     },
     "user_tz": -330
    },
    "id": "vhYMdNRyDxNw"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1) # (max_len, 1, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (seq_len, batch, d_model)\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1747130728993,
     "user": {
      "displayName": "surge",
      "userId": "10562721468625119372"
     },
     "user_tz": -330
    },
    "id": "_gtDpEP1EDgV"
   },
   "outputs": [],
   "source": [
    "class sps_Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(PRETRAINED_W2V2)\n",
    "        self.wav2vec2_encoder = Wav2Vec2Model.from_pretrained(PRETRAINED_W2V2)\n",
    "\n",
    "        if FREEZE_ENCODER:\n",
    "            for param in self.wav2vec2_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(TRANSFORMER_D_MODEL, TRANSFORMER_DROPOUT)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=TRANSFORMER_D_MODEL,\n",
    "            nhead=TRANSFORMER_NHEAD,\n",
    "            dim_feedforward=TRANSFORMER_DIM_FEEDFORWARD,\n",
    "            dropout=TRANSFORMER_DROPOUT,\n",
    "            batch_first=False # expecting (S, N, E)\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=TRANSFORMER_NUM_ENCODER_LAYERS\n",
    "        )\n",
    "\n",
    "        self.heads = nn.ModuleDict()\n",
    "        head_common_input_dim = TRANSFORMER_D_MODEL\n",
    "\n",
    "        for task_name, task_info in TASKS.items():\n",
    "            head_specific_config = HEAD_CONFIGS[task_name]\n",
    "            head_hidden_dim = head_specific_config['head_hidden_dim']\n",
    "            head_dropout_rate = head_specific_config['head_dropout_rate'] # Using this for head's dropout\n",
    "            output_dim = 1 if task_info['type'] == 'regression' else task_info.get('num_classes')\n",
    "            if output_dim is None and task_info['type'] == 'classification':\n",
    "                print(f\"num_classes not set for classification task '{task_name}'\")\n",
    "\n",
    "            self.heads[task_name] = nn.Sequential(\n",
    "                nn.Linear(head_common_input_dim, head_hidden_dim), nn.ReLU(),\n",
    "                nn.Dropout(head_dropout_rate), nn.Linear(head_hidden_dim, output_dim)\n",
    "            )\n",
    "\n",
    "    def forward(self, waveform):\n",
    "        current_device = next(self.parameters()).device\n",
    "        waveform_list = [wav.cpu().numpy() for wav in waveform] if waveform.ndim == 2 else [waveform.cpu().numpy()]\n",
    "\n",
    "        inputs = self.feature_extractor(\n",
    "            waveform_list, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\",\n",
    "            padding=\"longest\", return_attention_mask=True\n",
    "        )\n",
    "        inputs = {k: v.to(current_device) for k, v in inputs.items()}\n",
    "        attention_mask_input = inputs.get('attention_mask') # (N, S_in)\n",
    "        if attention_mask_input is None:\n",
    "            attention_mask_input = torch.ones(inputs['input_values'].shape[0], inputs['input_values'].shape[1], dtype=torch.long, device=current_device)\n",
    "\n",
    "        w2v2_outputs = self.wav2vec2_encoder(inputs['input_values'], attention_mask=attention_mask_input)\n",
    "        hidden_states = w2v2_outputs.last_hidden_state # (N, S_out, E) where E is TRANSFORMER_D_MODEL\n",
    "\n",
    "        # TransformerEncoder expects (S_out, N, E)\n",
    "        transformer_input = hidden_states.transpose(0, 1) # (S_out, N, E)\n",
    "\n",
    "\n",
    "        transformer_input = self.pos_encoder(transformer_input)\n",
    "\n",
    "\n",
    "        # Create src_key_padding_mask for Transformer: (N, S_out)\n",
    "        # attention_mask_input is (N, S_in)\n",
    "        actual_sequence_length = hidden_states.shape[1] # S_out\n",
    "        if attention_mask_input.shape[1] >= actual_sequence_length:\n",
    "            output_attention_mask = attention_mask_input[:, :actual_sequence_length] # (N, S_out), 1 for valid, 0 for pad\n",
    "        else:\n",
    "            output_attention_mask = torch.ones(hidden_states.shape[0], actual_sequence_length, device=current_device, dtype=torch.long)\n",
    "\n",
    "        src_key_padding_mask = (output_attention_mask == 0) # (N, S_out), True for pad\n",
    "\n",
    "        transformer_output = self.transformer_encoder(\n",
    "            transformer_input,\n",
    "            src_key_padding_mask=src_key_padding_mask\n",
    "        ) # (S_out, N, E)\n",
    "\n",
    "        # (N, S_out, E) for pooling\n",
    "        transformer_output_permuted = transformer_output.transpose(0, 1) # (N, S_out, E)\n",
    "\n",
    "        # Pooling\n",
    "        expanded_output_attention_mask = output_attention_mask.unsqueeze(-1).expand_as(transformer_output_permuted) # (N, S_out, E)\n",
    "\n",
    "        masked_transformer_output = transformer_output_permuted * expanded_output_attention_mask\n",
    "        summed_transformer_output = torch.sum(masked_transformer_output, dim=1) # (N, E)\n",
    "\n",
    "        # (N, 1)\n",
    "        valid_lengths = output_attention_mask.sum(dim=1, keepdim=True).clamp(min=1e-9) # (N, 1)\n",
    "        pooled_output = summed_transformer_output / valid_lengths # (N, E)\n",
    "\n",
    "        predictions = {}\n",
    "        for task_name, head_module in self.heads.items():\n",
    "            task_prediction = head_module(pooled_output)\n",
    "            if TASKS[task_name]['type'] == 'regression':\n",
    "                predictions[task_name] = task_prediction.squeeze(-1)\n",
    "            else:\n",
    "                predictions[task_name] = task_prediction\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 393,
     "status": "ok",
     "timestamp": 1747130729387,
     "user": {
      "displayName": "surge",
      "userId": "10562721468625119372"
     },
     "user_tz": -330
    },
    "id": "ECknOZocEQjP",
    "outputId": "b4e957d6-52c1-4c11-cdba-31a6b4d37017"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {DEVICE}\")\n",
    "random.seed(VAL_SPLIT_SEED)\n",
    "np.random.seed(VAL_SPLIT_SEED)\n",
    "torch.manual_seed(VAL_SPLIT_SEED)\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "  torch.cuda.manual_seed_all(VAL_SPLIT_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1193,
     "status": "ok",
     "timestamp": 1747130730582,
     "user": {
      "displayName": "surge",
      "userId": "10562721468625119372"
     },
     "user_tz": -330
    },
    "id": "mlPoNeafEXD5",
    "outputId": "7db3b7f8-9800-41d7-85bc-e96938e1677e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw train data: 4490 rows, raw test data: 1640 rows\n",
      "Processed train data: 4490 rows, Processed test data: 1640 rows\n"
     ]
    }
   ],
   "source": [
    "full_train_df_raw = pd.read_csv(TRAIN_CSV_PATH)\n",
    "test_df_raw = pd.read_csv(TEST_CSV_PATH)\n",
    "print(f\"Loaded raw train data: {len(full_train_df_raw)} rows, raw test data: {len(test_df_raw)} rows\")\n",
    "\n",
    "cols_to_drop = ['index', 'Use_x', 'DR', 'Ethnicity']\n",
    "full_train_df = full_train_df_raw.drop(columns=[c for c in cols_to_drop if c in full_train_df_raw.columns], errors='ignore')\n",
    "test_df = test_df_raw.drop(columns=[c for c in cols_to_drop if c in test_df_raw.columns], errors='ignore')\n",
    "\n",
    "critical_cols = ['FilePath'] + list(TASKS.keys())\n",
    "full_train_df.dropna(subset=critical_cols, inplace=True)\n",
    "test_df.dropna(subset=critical_cols, inplace=True)\n",
    "full_train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Processed train data: {len(full_train_df)} rows, Processed test data: {len(test_df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1747130730595,
     "user": {
      "displayName": "surge",
      "userId": "10562721468625119372"
     },
     "user_tz": -330
    },
    "id": "lkUqIw_KEaS7",
    "outputId": "e056c8c4-042e-421b-9156-88cd738f4373"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating normalization stats and mappings...\n",
      "  Age stats: Mean=30.29, Std=7.77\n",
      "  Gender mapping: {'F': 0, 'M': 1}, Num Classes: 2\n",
      "  Height stats: Mean=175.75, Std=9.52\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating normalization stats and mappings...\")\n",
    "for task_name, task_info in TASKS.items():\n",
    "    if task_info['type'] == 'regression':\n",
    "        mean, std = full_train_df[task_name].astype(float).mean(), full_train_df[task_name].astype(float).std()\n",
    "        NORM_STATS[task_name]['mean'], NORM_STATS[task_name]['std'] = mean, std if (np.isfinite(std) and std > 1e-6) else 1.0\n",
    "        print(f\"  {task_name.capitalize()} stats: Mean={NORM_STATS[task_name]['mean']:.2f}, Std={NORM_STATS[task_name]['std']:.2f}\")\n",
    "        \n",
    "    elif task_info['type'] == 'classification':\n",
    "        if task_name == 'Gender':\n",
    "            cats = sorted(list(full_train_df[task_name].astype(str).str.upper().unique()))\n",
    "            TASKS[task_name]['num_classes'] = len(cats)\n",
    "            GENDER_MAP = {cat: i for i, cat in enumerate(cats)}\n",
    "            print(f\"  Gender mapping: {GENDER_MAP}, Num Classes: {TASKS[task_name]['num_classes']}\")\n",
    "        else:\n",
    "            TASKS[task_name]['num_classes'] = len(full_train_df[task_name].unique())\n",
    "            print(f\"  {task_name.capitalize()} Num Classes: {TASKS[task_name]['num_classes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1747130730601,
     "user": {
      "displayName": "surge",
      "userId": "10562721468625119372"
     },
     "user_tz": -330
    },
    "id": "SfTakxvoEcQ-",
    "outputId": "ec0ab20f-8997-44b3-bbe8-7b8488ad6657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data by SpeakerID...\n",
      "Data split: Train=3360, Val=1130, Test=1640\n"
     ]
    }
   ],
   "source": [
    "if 'SpeakerID' in full_train_df.columns:\n",
    "    print(\"Splitting data by SpeakerID...\")\n",
    "    speaker_ids = full_train_df['SpeakerID'].unique()\n",
    "    train_spk_ids, val_spk_ids = train_test_split(speaker_ids, test_size=VAL_SPLIT_RATIO, random_state=VAL_SPLIT_SEED)\n",
    "    train_df, val_df = full_train_df[full_train_df['SpeakerID'].isin(train_spk_ids)].copy(), full_train_df[full_train_df['SpeakerID'].isin(val_spk_ids)].copy()\n",
    "\n",
    "else:\n",
    "    print(\"Warning: 'SpeakerID' not found. Performing random split.\")\n",
    "    train_df, val_df = train_test_split(full_train_df, test_size=VAL_SPLIT_RATIO, random_state=VAL_SPLIT_SEED)\n",
    "print(f\"Data split: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1747130730603,
     "user": {
      "displayName": "surge",
      "userId": "10562721468625119372"
     },
     "user_tz": -330
    },
    "id": "POJsEhVmEd8g"
   },
   "outputs": [],
   "source": [
    "train_dataset = TimitDataset(train_df, mode='train')\n",
    "val_dataset = TimitDataset(val_df, mode='eval')\n",
    "test_dataset = TimitDataset(test_df, mode='eval')\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=collate_fn, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312,
     "referenced_widgets": [
      "e407056fd60743c7adb9d8d06ce3ac9d",
      "4fbab73e743b4c89948b02c1cf00470a",
      "88e639d5c7ab416fa3937112858c6c01",
      "810c7ca2802b45228b4b8b8299fdb31f",
      "7155847ec1764e99b49e264d4bcde5cd",
      "260973fbd636461b88072eae7ddd14b2",
      "ae36d2d655a74e7f8f477c4bc081b7e4",
      "adc605d7659f4e38a7ea973204279e91",
      "242fd813c7f043fab6f4f057fbebec5d",
      "1dadb2f15f634b94a236e6c14c997bb5",
      "ad1f5fab9cfc4050af6da64524953c20",
      "0603e723fe5d457e83ad5064df62be4b",
      "d6c5c3300d7d4f659c7995802a64f357",
      "328c91884b394f97ad65d3b3ca5ad6b0",
      "07ab0f43d15e4b56b122aa2891a65f70",
      "493d40d4a2c54f31a9f5148ae955cc34",
      "e9b3fdf7e4c84f94bc2d1fd47386169c",
      "4a8916cf1650418786baad626a686d8d",
      "1b41180439fd42118bac04f5266531f2",
      "90f0c8946dc34cc3bc2958efdf621700",
      "08c9ed0cb2164e77bac531f5d2a15792",
      "a2e36427fce44532a338569cc17095f7",
      "31d345f4c218486c88cdeafcd79f4298",
      "72bfe43deac54c67a807ccc06415e751",
      "992267679dc54c4ba6250ed29fec04fb",
      "2f16886c49154d3ea9c9b5481cb2268e",
      "a22405357356480aa749fc8ecda97e90",
      "38ee50076e0c475a8b3bce92cfb8e534",
      "b1eda6aeaefe49db90bf780b5338541a",
      "ac97419bed294600a33ab13533f99b24",
      "cc0967db76b34d9394c9efc41e748670",
      "ca50e00510f549959c35d0d1a327ed6e",
      "3a645d70ba574a279793627e1a5a3eb3"
     ]
    },
    "executionInfo": {
     "elapsed": 4768,
     "status": "ok",
     "timestamp": 1747130735372,
     "user": {
      "displayName": "surge",
      "userId": "10562721468625119372"
     },
     "user_tz": -330
    },
    "id": "JsUz_HfmEjY9",
    "outputId": "cbb3aa2f-2927-4be0-80a9-f6f901146579"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e407056fd60743c7adb9d8d06ce3ac9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0603e723fe5d457e83ad5064df62be4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d345f4c218486c88cdeafcd79f4298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = sps_Transformer().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1747130735417,
     "user": {
      "displayName": "surge",
      "userId": "10562721468625119372"
     },
     "user_tz": -330
    },
    "id": "_D4M-Wj7Ekio"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE, weight_decay=OPTIMIZER_WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1747130735419,
     "user": {
      "displayName": "surge",
      "userId": "10562721468625119372"
     },
     "user_tz": -330
    },
    "id": "VDKmpGnJEhRi"
   },
   "outputs": [],
   "source": [
    "criterion_reg, criterion_cls = nn.MSELoss(), nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=MONITOR_METRIC_MODE, factor=LR_SCHEDULER_FACTOR, patience=LR_SCHEDULER_PATIENCE, min_lr=MIN_LR, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1747130735420,
     "user": {
      "displayName": "surge",
      "userId": "10562721468625119372"
     },
     "user_tz": -330
    },
    "id": "rfMpUyKjEo16"
   },
   "outputs": [],
   "source": [
    "def validate_epoch(val_loader, model, device):\n",
    "    model.eval()\n",
    "    running_task_losses_val = {task: 0.0 for task in TASKS}\n",
    "    num_samples_val = 0\n",
    "    criterion_reg_val, criterion_cls_val = nn.MSELoss(reduction='sum'), nn.CrossEntropyLoss(reduction='sum')\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(val_loader):\n",
    "            if batch_data is None: continue\n",
    "            wav, targets = batch_data\n",
    "            if wav.numel() == 0 or not targets: continue\n",
    "            wav, targets_device = wav.to(device), {k: v.to(device) for k, v in targets.items()}\n",
    "            current_batch_size = wav.size(0)\n",
    "            predictions = model(wav)\n",
    "            valid_batch = True\n",
    "            for task_name, task_info in TASKS.items():\n",
    "                if task_name not in predictions or task_name not in targets_device: \n",
    "                    valid_batch = False\n",
    "                    break\n",
    "                pred_val, target_val = predictions[task_name], targets_device[task_name]\n",
    "                if task_info['type'] == 'regression' and pred_val.shape != target_val.shape: target_val = target_val.view_as(pred_val)\n",
    "                loss_val = criterion_reg_val(pred_val, target_val) if task_info['type'] == 'regression' else criterion_cls_val(pred_val, target_val)\n",
    "                if torch.isnan(loss_val) or torch.isinf(loss_val): \n",
    "                    running_task_losses_val[task_name] += 1e9 * current_batch_size\n",
    "                    valid_batch = False\n",
    "                    break\n",
    "                else: \n",
    "                    running_task_losses_val[task_name] += loss_val.item()\n",
    "            if not valid_batch: \n",
    "                continue\n",
    "            num_samples_val += current_batch_size\n",
    "    avg_task_losses_val, weighted_val_loss = {}, 0.0\n",
    "    if num_samples_val > 0:\n",
    "        for task_name in TASKS.keys():\n",
    "            avg_loss = running_task_losses_val[task_name] / num_samples_val\n",
    "            avg_task_losses_val[task_name] = avg_loss\n",
    "            if task_name in EVAL_LOSS_WEIGHTS: weighted_val_loss += EVAL_LOSS_WEIGHTS[task_name] * avg_loss\n",
    "    else:\n",
    "        for task_name in TASKS.keys(): avg_task_losses_val[task_name] = float('inf')\n",
    "        weighted_val_loss = float('inf')\n",
    "    return avg_task_losses_val, weighted_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1071182,
     "status": "ok",
     "timestamp": 1747131806603,
     "user": {
      "displayName": "surge",
      "userId": "10562721468625119372"
     },
     "user_tz": -330
    },
    "id": "TG4AW02vEuiQ",
    "outputId": "c6482820-0a44-4669-cc44-8bc440b41c22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n",
      "E1 Summary: AvgTrL: 0.8226 | Avg ageTrL: 0.9571 | Avg GenTrL: 0.6006 | Avg heiTrL: 0.9977\n",
      "E1 Validation: WValL: 0.9786 | Avg ageVaL: 1.2600 | Avg GenVaL: 0.6602 | Avg heiVaL: 1.0528 | LR: 3.00e-05\n",
      "  Best model saved! E1, ValM: 0.9786\n",
      "E2 Summary: AvgTrL: 0.8072 | Avg ageTrL: 0.9266 | Avg GenTrL: 0.5969 | Avg heiTrL: 0.9890\n",
      "E2 Validation: WValL: 0.9648 | Avg ageVaL: 1.2413 | Avg GenVaL: 0.6441 | Avg heiVaL: 1.0534 | LR: 3.00e-05\n",
      "  Best model saved! E2, ValM: 0.9648\n",
      "E3 Summary: AvgTrL: 0.8049 | Avg ageTrL: 0.9290 | Avg GenTrL: 0.5908 | Avg heiTrL: 0.9851\n",
      "E3 Validation: WValL: 0.9717 | Avg ageVaL: 1.2354 | Avg GenVaL: 0.6664 | Avg heiVaL: 1.0551 | LR: 3.00e-05\n",
      "  No improvement for 1 epoch(s). Best: 0.9648\n",
      "E4 Summary: AvgTrL: 0.8037 | Avg ageTrL: 0.9229 | Avg GenTrL: 0.5943 | Avg heiTrL: 0.9842\n",
      "E4 Validation: WValL: 0.9631 | Avg ageVaL: 1.2391 | Avg GenVaL: 0.6449 | Avg heiVaL: 1.0471 | LR: 3.00e-05\n",
      "  Best model saved! E4, ValM: 0.9631\n",
      "E5 Summary: AvgTrL: 0.8024 | Avg ageTrL: 0.9225 | Avg GenTrL: 0.5924 | Avg heiTrL: 0.9823\n",
      "E5 Validation: WValL: 0.9737 | Avg ageVaL: 1.2323 | Avg GenVaL: 0.6641 | Avg heiVaL: 1.0756 | LR: 3.00e-05\n",
      "  No improvement for 1 epoch(s). Best: 0.9631\n",
      "E6 Summary: AvgTrL: 0.8041 | Avg ageTrL: 0.9266 | Avg GenTrL: 0.5926 | Avg heiTrL: 0.9824\n",
      "E6 Validation: WValL: 0.9689 | Avg ageVaL: 1.2327 | Avg GenVaL: 0.6611 | Avg heiVaL: 1.0570 | LR: 3.00e-05\n",
      "  No improvement for 2 epoch(s). Best: 0.9631\n",
      "E7 Summary: AvgTrL: 0.8030 | Avg ageTrL: 0.9222 | Avg GenTrL: 0.5925 | Avg heiTrL: 0.9855\n",
      "E7 Validation: WValL: 0.9720 | Avg ageVaL: 1.2365 | Avg GenVaL: 0.6567 | Avg heiVaL: 1.0737 | LR: 3.00e-05\n",
      "  No improvement for 3 epoch(s). Best: 0.9631\n",
      "E8 Summary: AvgTrL: 0.8024 | Avg ageTrL: 0.9237 | Avg GenTrL: 0.5910 | Avg heiTrL: 0.9823\n",
      "E8 Validation: WValL: 0.9694 | Avg ageVaL: 1.2333 | Avg GenVaL: 0.6533 | Avg heiVaL: 1.0738 | LR: 3.00e-05\n",
      "  No improvement for 4 epoch(s). Best: 0.9631\n",
      "E9 Summary: AvgTrL: 0.8020 | Avg ageTrL: 0.9219 | Avg GenTrL: 0.5921 | Avg heiTrL: 0.9822\n",
      "E9 Validation: WValL: 0.9723 | Avg ageVaL: 1.2381 | Avg GenVaL: 0.6511 | Avg heiVaL: 1.0831 | LR: 3.00e-05\n",
      "  No improvement for 5 epoch(s). Best: 0.9631\n",
      "E10 Summary: AvgTrL: 0.8020 | Avg ageTrL: 0.9221 | Avg GenTrL: 0.5912 | Avg heiTrL: 0.9833\n",
      "E10 Validation: WValL: 0.9718 | Avg ageVaL: 1.2356 | Avg GenVaL: 0.6579 | Avg heiVaL: 1.0719 | LR: 3.00e-05\n",
      "  No improvement for 6 epoch(s). Best: 0.9631\n",
      "E11 Summary: AvgTrL: 0.8011 | Avg ageTrL: 0.9220 | Avg GenTrL: 0.5896 | Avg heiTrL: 0.9823\n",
      "E11 Validation: WValL: 0.9708 | Avg ageVaL: 1.2355 | Avg GenVaL: 0.6571 | Avg heiVaL: 1.0687 | LR: 3.00e-06\n",
      "  No improvement for 7 epoch(s). Best: 0.9631\n",
      "E12 Summary: AvgTrL: 0.8011 | Avg ageTrL: 0.9214 | Avg GenTrL: 0.5902 | Avg heiTrL: 0.9825\n",
      "E12 Validation: WValL: 0.9702 | Avg ageVaL: 1.2361 | Avg GenVaL: 0.6558 | Avg heiVaL: 1.0671 | LR: 3.00e-06\n",
      "  No improvement for 8 epoch(s). Best: 0.9631\n",
      "E13 Summary: AvgTrL: 0.8012 | Avg ageTrL: 0.9221 | Avg GenTrL: 0.5887 | Avg heiTrL: 0.9840\n",
      "E13 Validation: WValL: 0.9699 | Avg ageVaL: 1.2362 | Avg GenVaL: 0.6555 | Avg heiVaL: 1.0660 | LR: 3.00e-06\n",
      "  No improvement for 9 epoch(s). Best: 0.9631\n",
      "E14 Summary: AvgTrL: 0.8009 | Avg ageTrL: 0.9224 | Avg GenTrL: 0.5889 | Avg heiTrL: 0.9818\n",
      "E14 Validation: WValL: 0.9693 | Avg ageVaL: 1.2355 | Avg GenVaL: 0.6545 | Avg heiVaL: 1.0665 | LR: 3.00e-06\n",
      "  No improvement for 10 epoch(s). Best: 0.9631\n",
      "Early stopping E14. No improvement for 10 epochs.\n",
      "--- Training Finished ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Starting Training ---\")\n",
    "best_val_metric = float('inf') if MONITOR_METRIC_MODE == 'min' else float('-inf')\n",
    "epochs_no_improve = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_loss_epoch = 0.0\n",
    "    task_losses_epoch = {task: 0.0 for task in TASKS}\n",
    "    num_samples_processed = 0\n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "        if batch_data is None: \n",
    "            continue\n",
    "        wav, targets = batch_data\n",
    "        if wav.numel() == 0 or not targets: \n",
    "            continue\n",
    "        wav, targets_device = wav.to(DEVICE), {k: v.to(DEVICE) for k, v in targets.items()}\n",
    "        current_batch_size = wav.size(0)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(wav)\n",
    "        combined_loss_batch = torch.tensor(0.0, device=DEVICE)\n",
    "        current_batch_task_losses = {}\n",
    "        valid_batch_for_loss = True\n",
    "        for task_name, task_info in TASKS.items():\n",
    "            if task_name not in predictions or task_name not in targets_device: \n",
    "                valid_batch_for_loss = False\n",
    "                break\n",
    "            pred, target = predictions[task_name], targets_device[task_name]\n",
    "            if task_info['type'] == 'regression' and pred.shape != target.shape: target = target.view_as(pred)\n",
    "            loss = criterion_reg(pred, target) if task_info['type'] == 'regression' else criterion_cls(pred, target)\n",
    "            if torch.isnan(loss) or torch.isinf(loss): \n",
    "                valid_batch_for_loss = False \n",
    "                break\n",
    "            combined_loss_batch += task_info['loss_weight'] * loss\n",
    "            current_batch_task_losses[task_name] = loss.item()\n",
    "\n",
    "        if valid_batch_for_loss:\n",
    "            combined_loss_batch.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRADIENT_CLIP_MAX_NORM) # Gradient Clipping\n",
    "            optimizer.step()\n",
    "            total_loss_epoch += combined_loss_batch.item() * current_batch_size\n",
    "            for task_name, loss_item in current_batch_task_losses.items():\n",
    "                task_losses_epoch[task_name] += loss_item * current_batch_size\n",
    "            num_samples_processed += current_batch_size\n",
    "            # if (batch_idx + 1) % 50 == 0:\n",
    "            #      print(f\"  E{epoch} B{batch_idx+1}/{len(train_loader)} | BLoss: {combined_loss_batch.item():.3f} | \" + \\\n",
    "            #            \" | \".join([f\"{k[:3]}L: {v:.3f}\" for k,v in current_batch_task_losses.items()]))\n",
    "\n",
    "    avg_loss_epoch = total_loss_epoch/num_samples_processed if num_samples_processed > 0 else float('inf')\n",
    "    avg_task_train_losses = {k:v/num_samples_processed if num_samples_processed > 0 else float('inf') for k,v in task_losses_epoch.items()}\n",
    "    print(f\"Epoch{epoch} Summary: AvgTrL: {avg_loss_epoch:.4f} | \" + \\\n",
    "          \" | \".join([f\"Avg {k[:3]}TrL: {v:.4f}\" for k,v in avg_task_train_losses.items()]))\n",
    "\n",
    "    if len(val_loader) > 0:\n",
    "        avg_task_val_losses, current_val_metric = validate_epoch(val_loader, model, DEVICE)\n",
    "        val_loss_str = \" | \".join([f\"Avg {k[:3]}VaL: {v:.4f}\" for k,v in avg_task_val_losses.items()])\n",
    "        print(f\"E{epoch} Validation: WValL: {current_val_metric:.4f} | {val_loss_str} | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        scheduler.step(current_val_metric)\n",
    "        if (MONITOR_METRIC_MODE == 'min' and current_val_metric < best_val_metric) or \\\n",
    "           (MONITOR_METRIC_MODE == 'max' and current_val_metric > best_val_metric):\n",
    "            best_val_metric, epochs_no_improve = current_val_metric, 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model_state, CKPT_PATH)\n",
    "            print(f\"  Best model saved! E{epoch}, ValM: {best_val_metric:.4f}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"  No improvement for {epochs_no_improve} epoch(s). Best: {best_val_metric:.4f}\")\n",
    "        if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"Early stopping E{epoch}. No improvement for {EARLY_STOPPING_PATIENCE} epochs.\")\n",
    "            break\n",
    "\n",
    "    else: \n",
    "        print(\"Skipping validation.\")\n",
    "print(\"--- Training Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1747131806647,
     "user": {
      "displayName": "surge",
      "userId": "10562721468625119372"
     },
     "user_tz": -330
    },
    "id": "7QYQHKhWExJb",
    "outputId": "9962a887-5ce5-49b9-b220-30834b0ab649"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating on Test Set with Best Model (Transformer) ---\n",
      "Loading best model from memory (ValM: 0.9631)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Evaluating on Test Set with Best Model (Transformer) ---\")\n",
    "if best_model_state is not None:\n",
    "    print(f\"Loading best model from memory (ValM: {best_val_metric:.4f})\")\n",
    "    model.load_state_dict(best_model_state)\n",
    "elif os.path.exists(CKPT_PATH):\n",
    "    print(f\"Loading best model from checkpoint: {CKPT_PATH}\")\n",
    "    model.load_state_dict(torch.load(CKPT_PATH, map_location=DEVICE))\n",
    "else:\n",
    "  print(\"No best model state found. Evaluating with the last model state.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14788,
     "status": "ok",
     "timestamp": 1747131821436,
     "user": {
      "displayName": "surge",
      "userId": "10562721468625119372"
     },
     "user_tz": -330
    },
    "id": "6w4sLFy-E0GH",
    "outputId": "86d97c0d-8a73-4867-9db7-d92c11a27a38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluated test batch 20/52\n",
      "  Evaluated test batch 40/52\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_targets_test, all_preds_test = {t:[] for t in TASKS}, {t:[] for t in TASKS}\n",
    "running_task_losses_test, num_samples_test = {t:0.0 for t in TASKS}, 0\n",
    "crit_reg_test, crit_cls_test = nn.MSELoss(reduction='sum'), nn.CrossEntropyLoss(reduction='sum')\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch_data in enumerate(test_loader):\n",
    "        if batch_data is None: continue\n",
    "        wav, targets = batch_data\n",
    "        if wav.numel() == 0 or not targets: continue\n",
    "        wav, targets_dev = wav.to(DEVICE), {k:v.to(DEVICE) for k,v in targets.items()}\n",
    "        current_batch_size = wav.size(0)\n",
    "        predictions = model(wav)\n",
    "        for task_name, task_info in TASKS.items():\n",
    "            if task_name not in predictions or task_name not in targets_dev: continue\n",
    "            pred_v, target_v = predictions[task_name], targets_dev[task_name]\n",
    "            if task_info['type'] == 'regression' and pred_v.shape != target_v.shape: target_v=target_v.view_as(pred_v)\n",
    "            loss_v = crit_reg_test(pred_v, target_v) if task_info['type']=='regression' else crit_cls_test(pred_v, target_v)\n",
    "            running_task_losses_test[task_name] += loss_v.item()\n",
    "            pred_cpu, targ_cpu = pred_v.cpu(), targets[task_name]\n",
    "            if task_info['type'] == 'regression':\n",
    "                m, s = NORM_STATS[task_name]['mean'], NORM_STATS[task_name]['std']\n",
    "                pred_denorm = (pred_cpu * (s if s > 1e-6 else 1.0)) + m\n",
    "                targ_denorm = (targ_cpu * (s if s > 1e-6 else 1.0)) + m\n",
    "                all_preds_test[task_name].extend(pred_denorm.tolist())\n",
    "                all_targets_test[task_name].extend(targ_denorm.tolist())\n",
    "            else:\n",
    "                all_preds_test[task_name].extend(torch.argmax(pred_cpu, dim=1).tolist())\n",
    "                all_targets_test[task_name].extend(targ_cpu.tolist())\n",
    "        num_samples_test += current_batch_size\n",
    "        if (batch_idx + 1) % 20 == 0:\n",
    "          print(f\"  Evaluated test batch {batch_idx+1}/{len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1747131821462,
     "user": {
      "displayName": "surge",
      "userId": "10562721468625119372"
     },
     "user_tz": -330
    },
    "id": "ekkD9MDd42x8",
    "outputId": "9f54d628-aa2c-4644-8a13-54825a9aa763"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Test Results (Best Transformer Model) ---\n",
      "  Avg Test Loss (age): 1.2079\n",
      "  Test MSE (age): 72.9151 (RMSE: 8.5390)\n",
      "  Avg Test Loss (Gender): 0.6424\n",
      "  Test Accuracy (Gender): 0.6585\n",
      "  Avg Test Loss (height): 0.9039\n",
      "  Test MSE (height): 81.9962 (RMSE: 9.0552)\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "metrics_test, avg_task_losses_test = {}, {}\n",
    "print(\"\\n--- Final Test Results (Best Transformer Model) ---\")\n",
    "if num_samples_test > 0:\n",
    "    for task_name, task_info in TASKS.items():\n",
    "        avg_task_losses_test[task_name] = running_task_losses_test[task_name]/num_samples_test\n",
    "        print(f\"  Avg Test Loss ({task_name}): {avg_task_losses_test[task_name]:.4f}\")\n",
    "        targets_np, preds_np = np.array(all_targets_test[task_name]), np.array(all_preds_test[task_name])\n",
    "        if len(targets_np) > 0 and len(targets_np) == len(preds_np):\n",
    "            if task_info['type'] == 'regression':\n",
    "                mse = mean_squared_error(targets_np, preds_np)\n",
    "                metrics_test[f\"{task_name}_mse\"] = mse\n",
    "                print(f\"  Test MSE ({task_name}): {mse:.4f} (RMSE: {np.sqrt(mse):.4f})\")\n",
    "            else:\n",
    "                acc = accuracy_score(targets_np, preds_np)\n",
    "                metrics_test[f\"{task_name}_acc\"] = acc\n",
    "                print(f\"  Test Accuracy ({task_name}): {acc:.4f}\")\n",
    "        else: print(f\"  Could not calculate metric for {task_name}.\")\n",
    "else: print(\"No samples processed during final test evaluation.\")\n",
    "print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOTZVC94txKAJUIGJnueMYF",
   "gpuType": "L4",
   "mount_file_id": "18IwqfxK3yhhpKblGKvvazaZCoIUhLm3O",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0603e723fe5d457e83ad5064df62be4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d6c5c3300d7d4f659c7995802a64f357",
       "IPY_MODEL_328c91884b394f97ad65d3b3ca5ad6b0",
       "IPY_MODEL_07ab0f43d15e4b56b122aa2891a65f70"
      ],
      "layout": "IPY_MODEL_493d40d4a2c54f31a9f5148ae955cc34"
     }
    },
    "07ab0f43d15e4b56b122aa2891a65f70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08c9ed0cb2164e77bac531f5d2a15792",
      "placeholder": "​",
      "style": "IPY_MODEL_a2e36427fce44532a338569cc17095f7",
      "value": " 1.60k/1.60k [00:00&lt;00:00, 200kB/s]"
     }
    },
    "08c9ed0cb2164e77bac531f5d2a15792": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b41180439fd42118bac04f5266531f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1dadb2f15f634b94a236e6c14c997bb5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "242fd813c7f043fab6f4f057fbebec5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "260973fbd636461b88072eae7ddd14b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f16886c49154d3ea9c9b5481cb2268e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca50e00510f549959c35d0d1a327ed6e",
      "placeholder": "​",
      "style": "IPY_MODEL_3a645d70ba574a279793627e1a5a3eb3",
      "value": " 378M/378M [00:00&lt;00:00, 482MB/s]"
     }
    },
    "31d345f4c218486c88cdeafcd79f4298": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_72bfe43deac54c67a807ccc06415e751",
       "IPY_MODEL_992267679dc54c4ba6250ed29fec04fb",
       "IPY_MODEL_2f16886c49154d3ea9c9b5481cb2268e"
      ],
      "layout": "IPY_MODEL_a22405357356480aa749fc8ecda97e90"
     }
    },
    "328c91884b394f97ad65d3b3ca5ad6b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b41180439fd42118bac04f5266531f2",
      "max": 1596,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_90f0c8946dc34cc3bc2958efdf621700",
      "value": 1596
     }
    },
    "38ee50076e0c475a8b3bce92cfb8e534": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a645d70ba574a279793627e1a5a3eb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "493d40d4a2c54f31a9f5148ae955cc34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a8916cf1650418786baad626a686d8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4fbab73e743b4c89948b02c1cf00470a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_260973fbd636461b88072eae7ddd14b2",
      "placeholder": "​",
      "style": "IPY_MODEL_ae36d2d655a74e7f8f477c4bc081b7e4",
      "value": "preprocessor_config.json: 100%"
     }
    },
    "7155847ec1764e99b49e264d4bcde5cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72bfe43deac54c67a807ccc06415e751": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_38ee50076e0c475a8b3bce92cfb8e534",
      "placeholder": "​",
      "style": "IPY_MODEL_b1eda6aeaefe49db90bf780b5338541a",
      "value": "model.safetensors: 100%"
     }
    },
    "810c7ca2802b45228b4b8b8299fdb31f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1dadb2f15f634b94a236e6c14c997bb5",
      "placeholder": "​",
      "style": "IPY_MODEL_ad1f5fab9cfc4050af6da64524953c20",
      "value": " 159/159 [00:00&lt;00:00, 17.0kB/s]"
     }
    },
    "88e639d5c7ab416fa3937112858c6c01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_adc605d7659f4e38a7ea973204279e91",
      "max": 159,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_242fd813c7f043fab6f4f057fbebec5d",
      "value": 159
     }
    },
    "90f0c8946dc34cc3bc2958efdf621700": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "992267679dc54c4ba6250ed29fec04fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac97419bed294600a33ab13533f99b24",
      "max": 377607901,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cc0967db76b34d9394c9efc41e748670",
      "value": 377607901
     }
    },
    "a22405357356480aa749fc8ecda97e90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2e36427fce44532a338569cc17095f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac97419bed294600a33ab13533f99b24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad1f5fab9cfc4050af6da64524953c20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "adc605d7659f4e38a7ea973204279e91": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae36d2d655a74e7f8f477c4bc081b7e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b1eda6aeaefe49db90bf780b5338541a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca50e00510f549959c35d0d1a327ed6e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc0967db76b34d9394c9efc41e748670": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d6c5c3300d7d4f659c7995802a64f357": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9b3fdf7e4c84f94bc2d1fd47386169c",
      "placeholder": "​",
      "style": "IPY_MODEL_4a8916cf1650418786baad626a686d8d",
      "value": "config.json: 100%"
     }
    },
    "e407056fd60743c7adb9d8d06ce3ac9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4fbab73e743b4c89948b02c1cf00470a",
       "IPY_MODEL_88e639d5c7ab416fa3937112858c6c01",
       "IPY_MODEL_810c7ca2802b45228b4b8b8299fdb31f"
      ],
      "layout": "IPY_MODEL_7155847ec1764e99b49e264d4bcde5cd"
     }
    },
    "e9b3fdf7e4c84f94bc2d1fd47386169c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
